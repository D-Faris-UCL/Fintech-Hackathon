{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr8IvAXmhbHg",
        "outputId": "07a262ed-83b4-40be-fa6e-613f4710f631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "<ipython-input-2-a33d5a7cc167>:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  stock_data.dropna(inplace=True)\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "<ipython-input-2-a33d5a7cc167>:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  stock_data.dropna(inplace=True)\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "<ipython-input-2-a33d5a7cc167>:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  stock_data.dropna(inplace=True)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - loss: 0.1468 - val_loss: 0.0712\n",
            "Epoch 2/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0134 - val_loss: 0.0176\n",
            "Epoch 3/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0092 - val_loss: 0.0217\n",
            "Epoch 4/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0051 - val_loss: 0.0051\n",
            "Epoch 5/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0033\n",
            "Epoch 6/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.0035\n",
            "Epoch 7/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0045\n",
            "Epoch 8/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0038\n",
            "Epoch 9/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0037\n",
            "Epoch 10/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0019 - val_loss: 0.0034\n",
            "Epoch 11/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0018 - val_loss: 0.0032\n",
            "Epoch 12/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0018 - val_loss: 0.0035\n",
            "Epoch 13/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0017 - val_loss: 0.0032\n",
            "Epoch 14/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0018 - val_loss: 0.0036\n",
            "Epoch 15/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0016 - val_loss: 0.0031\n",
            "Epoch 16/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0018 - val_loss: 0.0031\n",
            "Epoch 17/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0019 - val_loss: 0.0030\n",
            "Epoch 18/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0017 - val_loss: 0.0030\n",
            "Epoch 19/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0018 - val_loss: 0.0029\n",
            "Epoch 20/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0018 - val_loss: 0.0029\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "   Ticker  Week Ahead  Predicted Price  Price Growth\n",
            "0    AAPL           1       232.627686     -2.372314\n",
            "1    AAPL           2       233.931824     -1.068176\n",
            "2    AAPL           3       236.216248      1.216248\n",
            "3    AAPL           4       238.891495      3.891495\n",
            "4   GOOGL           1       165.347488      1.927490\n",
            "5   GOOGL           2       165.590515      2.170517\n",
            "6   GOOGL           3       164.700287      1.280289\n",
            "7   GOOGL           4       165.313522      1.893524\n",
            "8    MSFT           1       419.023376      0.863373\n",
            "9    MSFT           2       415.907104     -2.252899\n",
            "10   MSFT           3       416.281464     -1.878540\n",
            "11   MSFT           4       416.535919     -1.624084\n",
            "Plots have been saved as JPEG files in the current directory.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "import os  # To handle file paths\n",
        "\n",
        "# Step 1: Fetch data from Yahoo Finance\n",
        "def fetch_stock_data(tickers, period='5y'):\n",
        "    merged_data = pd.DataFrame()\n",
        "    for ticker in tickers:\n",
        "        try:\n",
        "            # Fetch historical data\n",
        "            stock_data = yf.download(ticker, period=period, interval='1wk')  # Weekly data\n",
        "            if stock_data.empty:\n",
        "                print(f\"No data fetched for {ticker}. Skipping.\")\n",
        "                continue\n",
        "            stock_data.reset_index(inplace=True)\n",
        "            # Ensure 'Date' column exists\n",
        "            if 'Date' not in stock_data.columns:\n",
        "                stock_data.rename(columns={'index': 'Date'}, inplace=True)\n",
        "            stock_data['Ticker'] = ticker\n",
        "            # Select necessary columns\n",
        "            stock_data = stock_data[['Date', 'Ticker', 'Close']]\n",
        "            # Drop rows with NaN values\n",
        "            stock_data.dropna(inplace=True)\n",
        "            merged_data = pd.concat([merged_data, stock_data], ignore_index=True)\n",
        "        except yf.shared.YFInvalidPeriodError as e:\n",
        "            print(f\"Invalid period error for {ticker}: {e}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while fetching data for {ticker}: {e}\")\n",
        "            continue\n",
        "    # Check if 'Ticker' column exists\n",
        "    if 'Ticker' not in merged_data.columns or merged_data.empty:\n",
        "        print(\"No data was fetched for any ticker. Exiting.\")\n",
        "        exit()\n",
        "    # Sort data\n",
        "    merged_data.sort_values(by=['Ticker', 'Date'], inplace=True)\n",
        "    return merged_data\n",
        "\n",
        "# Step 2: Prepare data\n",
        "def prepare_data(merged_data, lookback=4):\n",
        "    tickers = merged_data['Ticker'].unique()\n",
        "    data_list = []\n",
        "\n",
        "    for ticker in tickers:\n",
        "        ticker_data = merged_data[merged_data['Ticker'] == ticker].copy()\n",
        "        features = ticker_data[['Close']].values\n",
        "\n",
        "        if len(features) < lookback + 1:\n",
        "            print(f\"Not enough data for {ticker} after considering lookback. Skipping this ticker.\")\n",
        "            continue\n",
        "\n",
        "        for i in range(lookback, len(features)):\n",
        "            data_list.append({\n",
        "                'Ticker': ticker,\n",
        "                'X': features[i - lookback:i],\n",
        "                'y': features[i][0]\n",
        "            })\n",
        "\n",
        "    if not data_list:\n",
        "        raise ValueError(\"No data available after processing. Check the input data and lookback period.\")\n",
        "\n",
        "    data_df = pd.DataFrame(data_list)\n",
        "    return data_df\n",
        "\n",
        "# Step 3: Train LSTM model\n",
        "def train_lstm_model(data_df):\n",
        "    # Sort data to maintain time order\n",
        "    data_df = data_df.reset_index(drop=True)\n",
        "\n",
        "    # Split data into training, validation, and test sets\n",
        "    total_samples = len(data_df)\n",
        "    train_size = int(total_samples * 0.6)\n",
        "    val_size = int(total_samples * 0.2)\n",
        "\n",
        "    train_data = data_df.iloc[:train_size]\n",
        "    val_data = data_df.iloc[train_size:train_size + val_size]\n",
        "    test_data = data_df.iloc[train_size + val_size:]\n",
        "\n",
        "    # Prepare scaler using training data\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    # Stack the sequences for scaling\n",
        "    X_train = np.stack(train_data['X'].values)\n",
        "    y_train = train_data['y'].values\n",
        "\n",
        "    X_train_reshaped = X_train.reshape(-1, 1)\n",
        "    y_train_reshaped = y_train.reshape(-1, 1)\n",
        "\n",
        "    # Fit scaler on training data\n",
        "    scaler.fit(np.vstack((X_train_reshaped, y_train_reshaped)))\n",
        "\n",
        "    # Scale training data\n",
        "    X_train_scaled = scaler.transform(X_train_reshaped).reshape(X_train.shape)\n",
        "    y_train_scaled = scaler.transform(y_train_reshaped)\n",
        "\n",
        "    # Scale validation data\n",
        "    X_val = np.stack(val_data['X'].values)\n",
        "    y_val = val_data['y'].values\n",
        "    X_val_reshaped = X_val.reshape(-1, 1)\n",
        "    y_val_reshaped = y_val.reshape(-1, 1)\n",
        "    X_val_scaled = scaler.transform(X_val_reshaped).reshape(X_val.shape)\n",
        "    y_val_scaled = scaler.transform(y_val_reshaped)\n",
        "\n",
        "    # Scale test data\n",
        "    X_test = np.stack(test_data['X'].values)\n",
        "    y_test = test_data['y'].values\n",
        "    X_test_reshaped = X_test.reshape(-1, 1)\n",
        "    y_test_reshaped = y_test.reshape(-1, 1)\n",
        "    X_test_scaled = scaler.transform(X_test_reshaped).reshape(X_test.shape)\n",
        "    y_test_scaled = scaler.transform(y_test_reshaped)\n",
        "\n",
        "    # Build the LSTM model\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])),\n",
        "        tf.keras.layers.LSTM(32),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        X_train_scaled, y_train_scaled,\n",
        "        epochs=20, batch_size=32,\n",
        "        validation_data=(X_val_scaled, y_val_scaled)\n",
        "    )\n",
        "\n",
        "    return model, scaler\n",
        "\n",
        "# Step 4: Predict future stock prices\n",
        "def predict_future_prices(model, merged_data, scaler, future_weeks, lookback=4):\n",
        "    tickers = merged_data['Ticker'].unique()\n",
        "    predictions = {}\n",
        "\n",
        "    for ticker in tickers:\n",
        "        ticker_data = merged_data[merged_data['Ticker'] == ticker].copy()\n",
        "        features = ticker_data[['Close']].values\n",
        "\n",
        "        # Ensure there is enough data\n",
        "        if len(features) < lookback:\n",
        "            print(f\"Not enough data for {ticker} to make predictions. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Get the last 'lookback' data points\n",
        "        input_sequence = features[-lookback:]\n",
        "\n",
        "        # Scale the input sequence\n",
        "        input_sequence_scaled = scaler.transform(input_sequence)\n",
        "\n",
        "        ticker_predictions = []\n",
        "\n",
        "        # Predict future prices\n",
        "        for _ in range(future_weeks):\n",
        "            input_sequence_scaled = input_sequence_scaled.reshape(1, lookback, 1)\n",
        "            predicted_price_scaled = model.predict(input_sequence_scaled)\n",
        "            predicted_price = scaler.inverse_transform(predicted_price_scaled)[0][0]\n",
        "            ticker_predictions.append(predicted_price)\n",
        "\n",
        "            # Update the input sequence\n",
        "            input_sequence = np.append(input_sequence[1:], [[predicted_price]], axis=0)\n",
        "            input_sequence_scaled = scaler.transform(input_sequence)\n",
        "\n",
        "        predictions[ticker] = ticker_predictions\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Step 5: Calculate price growth\n",
        "def calculate_price_growth(merged_data, predictions):\n",
        "    price_growth_list = []\n",
        "\n",
        "    for ticker, ticker_predictions in predictions.items():\n",
        "        last_close_price = merged_data[merged_data['Ticker'] == ticker]['Close'].iloc[-1]\n",
        "\n",
        "        for week_ahead, predicted_price in enumerate(ticker_predictions, start=1):\n",
        "            growth = predicted_price - last_close_price\n",
        "            price_growth_list.append({\n",
        "                'Ticker': ticker,\n",
        "                'Week Ahead': week_ahead,\n",
        "                'Predicted Price': predicted_price,\n",
        "                'Price Growth': growth\n",
        "            })\n",
        "\n",
        "    price_growth_df = pd.DataFrame(price_growth_list)\n",
        "    return price_growth_df\n",
        "\n",
        "# Step 6: Plot historical data and predictions, and save as JPEG\n",
        "def plot_historical_and_predictions(merged_data, price_growth_df, future_weeks):\n",
        "    tickers = price_growth_df['Ticker'].unique()\n",
        "\n",
        "    for ticker in tickers:\n",
        "        # Filter data for the current ticker\n",
        "        ticker_data = merged_data[merged_data['Ticker'] == ticker]\n",
        "\n",
        "        # Get the last 5 years of historical data\n",
        "        last_five_years_date = ticker_data['Date'].max() - pd.DateOffset(years=5)\n",
        "        ticker_data_last_five_years = ticker_data[ticker_data['Date'] >= last_five_years_date]\n",
        "\n",
        "        # Prepare data for plotting\n",
        "        dates = list(ticker_data_last_five_years['Date'])\n",
        "        prices = list(ticker_data_last_five_years['Close'])\n",
        "\n",
        "        # Prepare future dates for predictions\n",
        "        last_date = ticker_data['Date'].max()\n",
        "        future_dates = [last_date + pd.DateOffset(weeks=1 * i) for i in range(1, future_weeks + 1)]\n",
        "        predicted_prices = price_growth_df[price_growth_df['Ticker'] == ticker]['Predicted Price'].values\n",
        "\n",
        "        # Plot historical prices\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(dates, prices, label='Historical Prices', color='blue')\n",
        "\n",
        "        # Plot predicted prices\n",
        "        plt.plot(future_dates, predicted_prices, marker='o', linestyle='--', label='Predicted Prices', color='red')\n",
        "\n",
        "        plt.title(f'Historical and Predicted Prices for {ticker}')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Price')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Save the plot as a JPEG file\n",
        "        filename = f\"{ticker}_historical_and_predicted_prices.jpg\"\n",
        "        plt.savefig(filename, format='jpg')\n",
        "        plt.close()  # Close the figure to free memory\n",
        "\n",
        "        # Plot price growth\n",
        "        ticker_growth_data = price_growth_df[price_growth_df['Ticker'] == ticker]\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(ticker_growth_data['Week Ahead'], ticker_growth_data['Price Growth'], marker='o', color='green')\n",
        "        plt.title(f'Predicted Price Growth for {ticker} Over Next {future_weeks} Weeks')\n",
        "        plt.xlabel('Week Ahead')\n",
        "        plt.ylabel('Price Growth')\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Save the price growth plot as a JPEG file\n",
        "        filename_growth = f\"{ticker}_predicted_price_growth.jpg\"\n",
        "        plt.savefig(filename_growth, format='jpg')\n",
        "        plt.close()  # Close the figure to free memory\n",
        "\n",
        "    print(\"Plots have been saved as JPEG files in the current directory.\")\n",
        "    return filename\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the tickers and period\n",
        "    tickers = ['AAPL', 'GOOGL', 'MSFT']  # List of tickers to analyze\n",
        "    period = '5y'  # Period of historical data to fetch (5 years)\n",
        "\n",
        "    # Step 1: Fetch stock data\n",
        "    merged_data = fetch_stock_data(tickers, period)\n",
        "\n",
        "    # Check if any data was fetched\n",
        "    if merged_data.empty or 'Ticker' not in merged_data.columns:\n",
        "        print(\"No data was fetched. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # Update tickers list based on data fetched\n",
        "    tickers = merged_data['Ticker'].unique()\n",
        "\n",
        "    # Step 2: Prepare the data\n",
        "    data_df = prepare_data(merged_data, lookback=4)\n",
        "\n",
        "    if data_df.empty:\n",
        "        print(\"No data available after processing. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # Step 3: Train the model\n",
        "    model, scaler = train_lstm_model(data_df)\n",
        "\n",
        "    # Step 4: Predict future prices\n",
        "    future_weeks = 4  # Number of weeks to predict into the future\n",
        "    predictions = predict_future_prices(model, merged_data, scaler, future_weeks, lookback=4)\n",
        "\n",
        "    if not predictions:\n",
        "        print(\"No predictions were made. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # Step 5: Calculate price growth\n",
        "    price_growth_df = calculate_price_growth(merged_data, predictions)\n",
        "\n",
        "    # Display the results\n",
        "    print(price_growth_df)\n",
        "\n",
        "    # Step 6: Plot the historical data and predictions, and save the plots\n",
        "    plot_historical_and_predictions(merged_data, price_growth_df, future_weeks)\n"
      ]
    }
  ]
}